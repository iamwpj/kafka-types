{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Standardizing Data for Kafka Consumers\n",
    "\n",
    "_A single pipeline approach to data standardization and serialization._\n",
    "\n",
    "Wesley Jones – Distributed Systems and Middleware – CPRE 550"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Background\n",
    "\n",
    "Kafka is a publish and subscribe messaging system that is \"naturally\" clustered. A single node can hold one of two basic roles: \n",
    "* broker\n",
    "* controller\n",
    "\n",
    "These nodes will utilize either an offered consensus service (Apache ZooKeeper) or a built in system (KRaft, a Kafka version of Raft).\n",
    "\n",
    "In this way one or several nodes can establish a cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Data Serializers\n",
    "\n",
    "_Using Apache Avro_\n",
    "\n",
    "* Allows for custom definition of schema types using an IDL\n",
    "* Has native APIs for common langauges (yours truly, Python)\n",
    "* Data is encoded to a schema, outputted to bytecode and can be decoded with the original schema\n",
    "\n",
    "<img src=\"typical-pipeline.png\" alt=\"A common Kafka+Avro setup\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Components of Kafka\n",
    "\n",
    "* Controllers and brokers\n",
    "* Topics\n",
    "* Partitions\n",
    "* Keys\n",
    "* Producers and consumers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Problem\n",
    "\n",
    "There are _inherent_ inefficiencies in a data pipeline For example:\n",
    "\n",
    "\n",
    "1. Producer creates a record\n",
    "2. Kafka receives the data and stores it within a topic\n",
    "3. A consumer receives the message from a topic\n",
    "4. An application parses message for effective usage\n",
    "   1. This repeats for each application\n",
    "\n",
    "\n",
    "_The steps of 4 and 4.A can create additional overhead for consuming data!_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Objectives\n",
    "\n",
    "1. Design a system that runs on top of Kafka and ensures data offered to consumers is standardized based on the content of data ingested.\n",
    "2. Analyze each message so that all data within a defined Kafka topic can be consumed with the same technique.\n",
    "3. Construct this to happen within the Kafka cluster so that no external services (aside from producers and consumers) are required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# About Related Works\n",
    "\n",
    "* Kafka was introduced by an engineer in 2011 at LinkedIn as an efficient centralized online and offline log processing cluster. The initial engineers have provided a good basis of foundational research on this process.\n",
    "* Many researchers have verified performance between Kafka and similar products such as RabbitMQ.\n",
    "* As a log ingest system some of Kafka's weaknesses with commit orders and general streaming inconsistencies are less important.\n",
    "* Data serialization as an add-on to a messaging system is a well established topic, there are some performance considerations I take into account in my system design.\n",
    "* Online log parsing efficiency was a point of interest as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Cluster\n",
    "\n",
    "I set up the cluster in Docker containers. This appeared to have no performance impact during the amount of testing I was doing. To collect metrics the entire cluster consisted of the following containers:\n",
    "* 3 Kafka nodes, all elligble for controller and broker roles.\n",
    "* 1 metrics exporter (kafka-exporter)\n",
    "* 1 metrics collector (Prometheus)\n",
    "* 1 metrics grapher (Grafana)\n",
    "* 1 UI viewer (kafka-ui)\n",
    "\n",
    "These were run with Podman on a small RedHat Enterprise Linux 8.9 server using 2 CPU, 8GB RAM and less than 10G of disk space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Data Serialization\n",
    "\n",
    "This involves a two phase process\n",
    "\n",
    "1. Parsing data into standard fields\n",
    "2. Labelling the data and creating a schema to apply (and then applying it)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Phase 1: Parsing Data\n",
    "\n",
    "Data is parsed using grok -- a regex matching technique.\n",
    "\n",
    "* To increase _capability_ several patterns are tried, and the \"winning\" one is giving selected.\n",
    "\n",
    "#### Input Example\n",
    "\n",
    "```plain\n",
    "<15>April 12 19:32:44 powlowski2164 quas[5150]: The PNG pixel is down, synthesize the back-end bandwidth so we can transmit the HTTP firewall!\n",
    "```\n",
    "\n",
    "#### Output Example\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"timestamp\": \"April 12 19:32:44\",\n",
    "  \"timestamp8601\": null,\n",
    "  \"facility\": null,\n",
    "  \"priority\": null,\n",
    "  \"logsource\": \"powlowski2164\",\n",
    "  \"program\": \"quas\",\n",
    "  \"pid\": \"5150\",\n",
    "  \"message\": \"The PNG pixel is down, synthesize the back-end bandwidth so we can transmit the HTTP firewall!\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Phase 2: Creating and Applying a Schema\n",
    "\n",
    "Schemas are auto-generated and applied to the data before it's subitted to a topic for consumption.\n",
    "\n",
    "* The data is condensed and then identified. This ensures that common log types are encoded with the same schema.\n",
    "\n",
    "#### Prepared data:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"timestamp\": \"April 12 19:32:44\",\n",
    "    \"logsource\": \"powlowski2164\",\n",
    "    \"program\": \"quas\",\n",
    "    \"pid\": 5150,\n",
    "    \"message\": \"The PNG pixel is down, synthesize the back-end bandwidth so we can transmit the HTTP firewall!\"\n",
    "}\n",
    "```\n",
    "\n",
    "#### Schema bytecode:\n",
    "\n",
    "```plain\n",
    "b’\\x00\"April 12 19:32:44\\x00\\x1apowlowski2164\\ x00\\x08quas\\x00\\xbcP\\x00\\xbc\\x01The PNG pixel is down, synthesize the back-end bandwidth so we can transmit the HTTP firewall!’\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Resulting System Design\n",
    "\n",
    "![A \"single-stream\" pipeline – in that all the consumers pull data in the same process.](./redefined-pipeline.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Performance\n",
    "\n",
    "* Around 20% or less of realtime (bad)\n",
    "\n",
    "| Step | Time |\n",
    "| --- | --- |\n",
    "| Matching grok pattern | 1.97 × 10−3 seconds (1970μ) |\n",
    "| Schema application | 3.57 × 10−5 seconds (35μ) |\n",
    "| Producer | 2.45 × 10−5 seconds (24μ) |\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Parsing is the problem\n",
    "\n",
    "Message parsing is the major delay – taking over 50 times longer to complete than the second steps in the process.\n",
    "\n",
    "At no point did the Kafka processes (producer, broker operation, and consuming) play into performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Schemas\n",
    "\n",
    "The schema process was very fast and introduced an acceptable delay in to the chain. It would be worth it to retain Avro as a standarization tool for the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Improvements\n",
    "\n",
    "1. Find a faster parser\n",
    "2. Field enumeration for schema generation is inefficient\n",
    "3. Implement with C or Java"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conclusion & Future Work\n",
    "\n",
    "The system design for a single pipeline is likely good, but latency improvments must occur.\n",
    "\n",
    "* Online log parsing research for efficient streaming log parsing work is needed\n",
    "* A generalized preparsing framework for Apache Kafka\n",
    "* Apache Avro schema auto-generation tools could be created that are more robust and feature-full."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
